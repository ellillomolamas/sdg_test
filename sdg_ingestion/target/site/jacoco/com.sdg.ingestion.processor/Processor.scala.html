<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>Processor.scala</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">sdg_ingestion</a> &gt; <a href="index.source.html" class="el_package">com.sdg.ingestion.processor</a> &gt; <span class="el_source">Processor.scala</span></div><h1>Processor.scala</h1><pre class="source lang-java linenums">package com.sdg.ingestion.processor

import com.sdg.ingestion.config.IngestionSettings
import com.sdg.ingestion.workers.json.JSONTools
import com.sdg.ingestion.workers.pipeline.ETLTool
import org.apache.log4j.{Logger, PropertyConfigurator}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

<span class="nc" id="L10">class Processor(config: IngestionSettings) {</span>

<span class="nc" id="L12">  PropertyConfigurator.configure(config.log4jFile)</span>
<span class="nc" id="L13">  private[this] val logger = Logger.getLogger(this.getClass)</span>

<span class="nc" id="L15">  val sparkConf: SparkConf = new SparkConf().</span>
<span class="nc" id="L16">    setAppName(&quot;SDG-Test-Ingestion&quot;).</span>
<span class="nc" id="L17">    set(&quot;spark.sql.parquet.enableVectorizedReader&quot;, &quot;false&quot;)</span>

<span class="nc" id="L19">  val spark: SparkSession = SparkSession.builder().config(sparkConf).</span>
    enableHiveSupport().
<span class="nc" id="L21">    config(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;).</span>
    getOrCreate()

<span class="nc" id="L24">  spark.sqlContext.setConf(&quot;spark.sql.parquet.writeLegacyFormat&quot;, &quot;true&quot;)</span>

  def process(): Unit = {
<span class="nc" id="L27">    logger.info(&quot;-----------------&gt; Starting process&quot;)</span>
<span class="nc" id="L28">    val dataFlows = JSONTools.readJSON(config.dataflowPath)</span>
<span class="nc" id="L29">    logger.info(&quot;-----------------&gt; Input JSON: &quot; + JSONTools.printJSON(dataFlows) )</span>
<span class="nc" id="L30">    ETLTool.process(spark, dataFlows, config.useHdfs);</span>
<span class="nc" id="L31">    logger.info(&quot;-----------------&gt; Finishing Process&quot;)</span>
  }
}
<span class="nc" id="L34"></span>
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.3.201901230119</span></div></body></html>